# Attack and Defense

- Reference
  - [Machine Learning (2019,Spring)](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.html)
- Motivation
  - We seek to deploy machine learning classifiers not only in the labs, but also in real world.
  - The classifiers that are robust to noises and work "most of the time" is not sufficient.
  - We want the classifiers that are robust the inputs that are built to fool the classifier.
  - Especially useful for spam classification, malware detection, network intrusion detection, etc.
  
 ### Attack
 
 <div align=center>
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/images/life_long_learning_1.png" width="30%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/images/life_long_learning_2.png" width="30%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/images/life_long_learning_1.png" width="30%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/images/life_long_learning_2.png" width="30%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/images/life_long_learning_3.png" width="30%" /><br/>
  How to Attack
</div>

Explaining and harnessing adversarial examples \[2014, arxiv, Ian J Goodfellow\] \[[paper](https://arxiv.org/pdf/1412.6572.pdf))\]
 
Adversarial examples in the physical world \[2016, arxiv, Alexey Kurakin\] \[[paper](https://arxiv.org/pdf/1607.02533.pdf?utm_source=sciontist.com&utm_medium=refer&utm_campaign=promote)\]
 
Intriguing properties of neural networks \[2013, arxiv, Christian Szegedy\] \[[paper](https://arxiv.org/pdf/1312.6199.pdf?not-changed)\]
  
Deepfool: a simple and accurate method to fool deep neural networks \[2016, CVPR, Seyed-Mohsen Moosavi-Dezfooli\] \[[paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Moosavi-Dezfooli_DeepFool_A_Simple_CVPR_2016_paper.pdf)\]
   
The limitations of deep learning in adversarial settings \[2016, SP, Nicolas Papernot\] \[[paper](https://arxiv.org/pdf/1511.07528.pdf)\]
    
Towards evaluating the robustness of neural networks \[2017, SP, Nicholas Carlini\] \[[paper](https://arxiv.org/pdf/1608.04644)\]
     
Ead: elastic-net attacks to deep neural networks via adversarial examples \[2018, AAAI, Pin-Yu Chen\] \[[paper](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPDFInterstitial/16893/15665)\]
      
Spatially transformed adversarial examples \[2018, arxiv, Chaowei Xiao\] \[[paper](https://arxiv.org/pdf/1801.02612.pdf)\]
       
One pixel attack for fooling deep neural networks \[2019, TEC, Jiawei Su\] \[[paper](https://arxiv.org/pdf/1710.08864.pdf)\]
  
 
  
  
  
  
  
  

