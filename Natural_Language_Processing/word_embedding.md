# Word Embedding
`code`: [feature engineering text data](https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/bonus%20content/feature%20engineering%20text%20data)

### LSA

### pLSA

### LDA

### N-gram

### one-hot representation

### Co-Occurance Vector

###  NNLM
full name: Nerual Network Language Model

A neural probabilistic language model \[2003, JMLR, Yoshua Bengio\] \[[paper](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\]

### fastText

\[[webcite](https://fasttext.cc/docs/en/support.html)\] \[[code](https://github.com/facebookresearch/fastText)\]

### glove

Glove: Global vectors for word representation \[2014, EMNLP, Jeffrey Pennington\] \[[paper](https://www.aclweb.org/anthology/D14-1162.pdf)\]

\[[webcite](https://nlp.stanford.edu/projects/glove/)\] \[[code](https://github.com/stanfordnlp/GloVe)\] \[[blog](http://www.foldl.me/2014/glove-python/)\]

### word2vec
Word2Vec is a group of models that tries to represent each word in a large text as a vector in a space of N dimensions (which we will call features) making similar words also be close to each other.

Efficient estimation of word representations in vector space \[2013, arxiv, Tomas Mikolov\] \[[paper](https://arxiv.org/pdf/1301.3781.pdf%5D)\]

Distributed representations of words and phrases and their compositionality \[2013, NIPS, Tomas Mikolov\] \[[paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\]

Distributed representations of sentences and documents \[2014, JMLR, Quoc Le\] \[[paper](http://proceedings.mlr.press/v32/le14.pdf)\]

\[[webcite](https://code.google.com/archive/p/word2vec/)\]

#### Skip-gram
Skip-gram is one of the unsupervised learning techniques used to find the most related words for a given word.

`algorithm`: Skip-gram is used to predict the context word for a given target word. Itâ€™s reverse of CBOW algorithm. Here, target word is input while context words are output.

`code`: [Word2Vec (Part 1): NLP With Deep Learning with Tensorflow (Skip-gram)](http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/)

#### CBOW
full name: Continuous Bag of Words

### ELMO
full name: Embedding from Language Models

### GPT

### BERT
full name: Bidirectional Encoder Representations from Transformer

