# Word Embedding
`code`: [feature engineering text data](https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/bonus%20content/feature%20engineering%20text%20data)

### LSA

### pLSA

### LDA

### N-gram

### one-hot representation

### Co-Occurance Vector

###  NNLM
full name: Nerual Network Language Model

A neural probabilistic language model \[2003, JMLR, Yoshua Bengio\] \[[paper](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\]

### word2vec
Word2Vec is a group of models that tries to represent each word in a large text as a vector in a space of N dimensions (which we will call features) making similar words also be close to each other.

Efficient estimation of word representations in vector space \[2013, arxiv, Tomas Mikolov\] \[[paper](https://arxiv.org/pdf/1301.3781.pdf%5D)\]

Distributed representations of words and phrases and their compositionality \[2013, NIPS, Tomas Mikolov\] \[[paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\]

Distributed representations of sentences and documents \[2014, JMLR, Quoc Le\] \[[paper](http://proceedings.mlr.press/v32/le14.pdf)\]

word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method \[2014, arxiv, Yoav Goldberg\] \[[paper](https://arxiv.org/pdf/1402.3722.pdf)\]

word2vec parameter learning explained \[2014, arxiv, Xin Rong\] \[[paper](https://arxiv.org/pdf/1411.2738.pdf)\]

\[[webcite](https://code.google.com/archive/p/word2vec/)\] \[[code](https://github.com/tmikolov/word2vec)\]

#### Skip-gram
Skip-gram is one of the unsupervised learning techniques used to find the most related words for a given word.

`algorithm`: Skip-gram is used to predict the context word for a given target word. Itâ€™s reverse of CBOW algorithm. Here, target word is input while context words are output.

`code`: [Word2Vec (Part 1): NLP With Deep Learning with Tensorflow (Skip-gram)](http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/)

#### CBOW
full name: Continuous Bag of Words

### glove

Glove: Global vectors for word representation \[2014, EMNLP, Jeffrey Pennington\] \[[paper](https://www.aclweb.org/anthology/D14-1162.pdf)\]

\[[webcite](https://nlp.stanford.edu/projects/glove/)\] \[[code](https://github.com/stanfordnlp/GloVe)\] \[[blog](http://www.foldl.me/2014/glove-python/)\] \[[stanford notes](https://cs224d.stanford.edu/lecture_notes/notes1.pdf)\]

### fastText

Bag of tricks for efficient text classification \[2016, arxiv, Armand Joulin\] \[[paper](https://arxiv.org/pdf/1607.01759v2.pdf)\]

Enriching word vectors with subword information \[2017, ACL, Piotr Bojanowski\] \[[paper](https://arxiv.org/pdf/1607.04606v1.pdf)\]

\[[webcite](https://fasttext.cc/docs/en/support.html)\] \[[code](https://github.com/facebookresearch/fastText)\]

### ELMO
full name: Embedding from Language Models

Deep contextualized word representations \[2018, arxiv, Matthew E. Peters\] \[[paper](https://arxiv.org/pdf/1802.05365.pdf%E3%80%91)\]

### GPT
full name: Generative Pre-Training

Improving language understanding by generative pre-training \[2018, arxiv, Alec Radford\] \[[paper](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)\]

### BERT
full name: Bidirectional Encoder Representations from Transformer

Bert: Pre-training of deep bidirectional transformers for language understanding \[2018, arxiv, Jacob Devlin\] \[[paper](https://arxiv.org/pdf/1810.04805.pdf%E3%80%91)\]

