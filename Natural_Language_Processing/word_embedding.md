# Word Embedding
`code`: [feature engineering text data](https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/bonus%20content/feature%20engineering%20text%20data)

### LSA

### pLSA

### LDA

### N-gram

### one-hot representation

### Co-Occurance Vector

###  NNLM
full name: Nerual Network Language Model

### fasttext

\[[webcite](https://fasttext.cc/docs/en/support.html)\] \[[code](https://github.com/facebookresearch/fastText)\]

### glove

### word2vec
Word2Vec is a group of models that tries to represent each word in a large text as a vector in a space of N dimensions (which we will call features) making similar words also be close to each other.

#### Skip-gram
Skip-gram is one of the unsupervised learning techniques used to find the most related words for a given word.

`algorithm`: Skip-gram is used to predict the context word for a given target word. Itâ€™s reverse of CBOW algorithm. Here, target word is input while context words are output.

`code`: [Word2Vec (Part 1): NLP With Deep Learning with Tensorflow (Skip-gram)](http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/)

#### CBOW
full name: Continuous Bag of Words

### ELMO
full name: Embedding from Language Models

### GPT

### BERT
full name: Bidirectional Encoder Representations from Transformer

