# Word Embedding

#### Language Model

#### N-gram

#### one-hot representation

#### Co-Occurance Vector

####  NNLM
full name: Nerual Network Language Model

#### Distributed Representation

### word2vec
Word2Vec is a group of models that tries to represent each word in a large text as a vector in a space of N dimensions (which we will call features) making similar words also be close to each other.

#### Skip-gram
Skip-gram is one of the unsupervised learning techniques used to find the most related words for a given word.

`algorithm`: Skip-gram is used to predict the context word for a given target word. Itâ€™s reverse of CBOW algorithm. Here, target word is input while context words are output.

#### CBOW
full name: Continuous Bag of Words

#### ELMO
full name: Embedding from Language Models

#### Encoder-Decoder

#### Transformer

#### BERT
full name: Bidirectional Encoder Representations from Transformer

