# Self-attention

- Reference
  - [Machine Learning (2019,Spring)](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.html)

### self attention
<div align=center>
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/self_attention_1.png" width="50%" /><br/>
  Self-attention
</div>

<br/>

<div align=center>
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/self_attention_2.png" width="40%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/self_attention_3.png" width="20%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/self_attention_4.png" width="40%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/self_attention_5.png" width="40%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/self_attention_6.png" width="40%" /><br/>
  Self-attention
</div>

Attention is all you need \[2017, NIPS, Ashish Vaswani\] \[[paper](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)\]

### Multi-head Self-attention

<div align=center>
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/multi_head_self_attention_1.png" width="40%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/multi_head_self_attention_2.png" width="40%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/multi_head_self_attention_3.png" width="40%" /><br/>
  Multi-head Self-attention
</div>

### Positional Encoding

<div align=center>
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/positional_encoding_1.png" width="40%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/positional_encoding_2.png" width="15%" /><br/>
  Positional Encoding
</div>

<br/>

[e visualization](http://jalammar.github.io/illustrated-transformer/)

### Seq2seq with Attention
<div align=center>
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/seq2seq_with_attention.png" width="40%" /><br/>
  Seq2seq with Attention
</div>

### Transformer
[Google AI Blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)

<div align=center>
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/transformer.png" width="40%" /><br/>
  Transformer
</div>

### Universal Transformer

[Google AI Blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)

Self-attention generative adversarial networks \[2018, arxiv, Han Zhang\] \[[paper](https://arxiv.org/pdf/1805.08318.pdf)\]










