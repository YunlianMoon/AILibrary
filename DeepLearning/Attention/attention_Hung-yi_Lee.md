# Attention

- reference from [Machine Learning and having it deep and structured (2015,Fall)](http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2015_2/Lecture/Attain%20(v3).ecm.mp4/index.html) by Hung-yi Lee
  - Attention on Sensory Information
  - Attention on Memory
  
  
### Hung-yi Lee
#### Attention on Sensory Information
  
application:
  - Machine Translation
  - Speech Recognition
  - Image Caption Generation
  - Video Caption Generation
  - Reading Comprehension/Quesion Answering
  - Visual Question Answering
  
  
##### machine translation
  
<div align=center>
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/machine_translate_match.png" width="45%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/machine_translation_rnn_1.png" width="45%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/machine_translation_rnn_2.png" width="25%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/machine_translation_rnn_3.png" width="35%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/machine_translation_rnn_4.png" width="32%" /><br/>
  Attention based Machine Translation Model
</div>

#### speech recognition

Listen, attend and spell: A neural network for large vocabulary conversational speech recognition \[2016, ICASSP, William Chan\] \[[Paper](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/44926.pdf)\]

#### image caption generation

<div align=center>
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/image_caption_match.png" width="30%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/image_caption_rnn_1.png" width="30%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/image_caption_rnn_2.png" width="30%" /><br/>
  Attention based Image Caption Model
</div>

Show, attend and tell: Neural image caption generation with visual attention \[2015, ICML, Kelvin Xu\] \[[Paper](http://proceedings.mlr.press/v37/xuc15.pdf)\]

#### reading comprehension

<div align=center>
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/reading_comprehension.png" width="40%" /> <br/>
  Attention based Reading Comprehension Model
</div>

<br/>
<div align=center>
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/reading_comprehension_memory_network_1.png" width="40%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/reading_comprehension_memory_network_2.png" width="40%" /><br/>
  Attention based Reading Comprehension Model
</div>

End-to-end memory networks \[2015, NIPS, Sainbayar Sukhbaatar\] \[[Paper](https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf)\]

Spatial transformer networks \[2015, NIPS, Max Jaderberg\] \[[Paper](http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf)\]

#### Attention on Memory

<div align=center>
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/neural_turing_machine_1.png" width="40%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/neural_turing_machine_2.png" width="40%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/neural_turing_machine_3.png" width="40%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/neural_turing_machine_4.png" width="40%" /><br/>
  Neural Turing Machine
</div>

<br/>
<div align=center>
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/stack_rnn.png" width="50%" /><br/>
  Stack RNN
</div>

Neural turing machines \[2014, arxiv, Alex Graves\] \[[Paper](https://arxiv.org/pdf/1410.5401.pdf%20(http://Neural%20Turning%20Machines)%20)\]

Reinforcement learning neural turing machines-revised \[2015, arxiv, Wojciech Zaremba\] \[[Paper](https://arxiv.org/pdf/1505.00521.pdf?utm_content=buffer2aaa3&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer)\]

Inferring algorithmic patterns with stack-augmented recurrent nets \[2015, NIPS, Armand Joulin\] \[[Paper](https://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.pdf)\]

End-to-end memory networks \[2015, NIPS, Sainbayar Sukhbaatar\] \[[Paper](https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf)\]

---
<div align=center>
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/summary.png" width="50%" />
</div>

Ask me anything: Dynamic memory networks for natural language processing \[2016, ICML, Ankit Kumar\] \[[Paper](http://117.128.6.19/cache/proceedings.mlr.press/v48/kumar16.pdf?ich_args2=506-06223805024609_cc9a49a6346b57a963eb82b841345dfb_10001002_9c896125dfc0f0d8973f518939a83798_54e0ec0af538ef972e0245b27382d357)\]

Neural machine translation by jointly learning to align and translate \[2015, arxiv, Dzmitry Bahdanau\] \[[Paper](https://arxiv.org/pdf/1409.0473.pdf)\]

Attention-based models for speech recognition \[2015, NIPS, Jan K Chorowski\] \[[Paper](http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf)\]

A neural attention model for abstractive sentence summarization \[2015, arxiv, Alexander M. Rush\] \[[Paper](https://arxiv.org/pdf/1509.00685.pdf%C3%AF%C2%BC%E2%80%B0)\]
